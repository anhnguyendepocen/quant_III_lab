library(foreign)
library(rstan)
rstan_options(auto_write = TRUE)
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
#-----------------------------------------------------------------------------
# recap:
# Denis showed you how to run a ols and poisson regression in rstan
# now, we are going to do:
# 1. simple partial pooling (no regressors), with informative priors
# 2. Random intercepts model
#-----------------------------------------------------------------------------
# 1. Predicting baseball batting averages via a hierarchical model
# (classic examples that shows Bayesian models outperforming MLE)
# See pages 310-316 Jackman
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
# Function to convert stan fit objects to coda objects
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
data(EfronMorris)
d <- EfronMorris
# codebook:
?EfronMorris
# MLE estimate is just batting average
MLE_rmse <- sqrt(mean((d$y-d$p)^2))
MLE_rmse
sqrt(mean(d$y) * (1-(mean(d$y)))/45)
mean(d$y)
(1-(mean(d$y)
)
)
x <- seq(0, 0.5, 0.001)
plot(x, dnorm(x, mean=0.225, sd=0.0375), type="l")
sd(d$y)
x <- seq(0.001, 1, 0.001)
plot(x, dgamma(x, shape=7, rate=70), type="l")
#-----------------------------------------------------------------------------
# recap:
# Denis showed you how to run a ols and poisson regression in rstan
# now, we are going to do:
# 1. simple partial pooling (no regressors), with informative priors
# 2. Random intercepts model
#-----------------------------------------------------------------------------
# 1. Predicting baseball batting averages via a hierarchical model
# (classic examples that shows Bayesian models outperforming MLE)
# See pages 310-316 Jackman
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
# Function to convert stan fit objects to coda objects
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
data(EfronMorris)
d <- EfronMorris
# codebook:
?EfronMorris
# MLE estimate is just batting average
MLE_rmse <- sqrt(mean((d$y-d$p)^2))
MLE_rmse
# Hierarchical model ('partial pooling')
# each observed batting average y_i is drawn from a normal density with
# unknown mean theta_i; and fixed variance sigma:
sigma <- sqrt(mean(d$y) * (1-(mean(d$y)))/45)
# Each theta_i (the true batting average of each player; which we
# would observe  if they kept playing forever) is also drawn
# from a normal distribution with unknown mean mu and variance tau.
# Then we give priors to the hyperparameters mu and tau.
# for mu, we know it must be somewhere between 0.15 and .30; so we
# choose values that give a 95% credible interval: normal(0.225, 0.0375)
# see Jackson p.311 for Baseball domain knowledge...
x <- seq(0, 0.5, 0.001)
plot(x, dnorm(x, mean=0.225, sd=0.0375), type="l")
# for tau, we choose a gamma distribution (why? it's always positive
# and it's a conjugate prior for a normal distribution)
# what prior belief do we have about tau,
# the standard deviation of the thetas?
# the sd of the observed y is:
sd(d$y)
# it must be somewhere around 0.07; probably higher; let's say 0.10
# so MEAN of values we draw from gamma distribution must be around 0.10
# gamma distribution has mean shape/rate [OR shape*scale]
# so let's follow Jackman and choose shape=7; then rate=7/0.10 ~= 70
x <- seq(0.001, 1, 0.001)
plot(x, dgamma(x, shape=7, rate=70), type="l")
hierarchical_code <- '
data {
int<lower=0> N; # observations
real y[N]; # outcome variable
real<lower=0> sigma; # fixed variance of observed batting averages
}
parameters {
real theta[N]; ## unobserved true batting averages
real mu; ## hyperparameter: mean of batting averages
real<lower=0> tau; ## hyperparameter: variance of batting averages
}
model {
mu ~ normal(0.225, 0.0375); ## prior about hyperparameter (overall batting average)
tau ~ gamma(7, 70); ## priors about variance of overall batting average
for (n in 1:N){;
y[n] ~ normal(theta[n], sigma);
theta[n] ~ normal(mu, tau);
}
}
'
data <- list(N=length(d$y), y=d$y, sigma=sigma)
fit <- stan(model_code=hierarchical_code, data=data, iter=1000, chains=4)
fit
str(fit)
?stan
obj <- stan(model_code=hierarchical_code,save_dso=T)
fit <- stan(model_code=hierarchical_code, data=data, iter=1000, chains=4,save_dso=T)
fit
fit@stanmodel
fit2 <- stan(fit@stanmodel, data=data, iter=1000, chains=4,save_dso=T)
fit2 <- stan(fit = fit@stanmodel, data=data, iter=1000, chains=4,save_dso=T)
fit <- stan(model_code=hierarchical_code, data=data, iter=1000, chains=4)
fit <- stan(model_code=hierarchical_code, data=data, iter=10000, chains=4)
monitor(fit, digits_summary=3)
mcmc <- stan_to_coda(fit)
# Function to convert stan fit objects to coda objects
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
mcmc <- stan_to_coda(fit)
?mcmc.list
??mcmc.list
library(coda)
#-----------------------------------------------------------------------------
# recap:
# Denis showed you how to run a ols and poisson regression in rstan
# now, we are going to do:
# 1. simple partial pooling (no regressors), with informative priors
# 2. Random intercepts model
#-----------------------------------------------------------------------------
# 1. Predicting baseball batting averages via a hierarchical model
# (classic examples that shows Bayesian models outperforming MLE)
# See pages 310-316 Jackman
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
# Function to convert stan fit objects to coda objects
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
data(EfronMorris)
d <- EfronMorris
# codebook:
?EfronMorris
# MLE estimate is just batting average
MLE_rmse <- sqrt(mean((d$y-d$p)^2))
MLE_rmse
# Hierarchical model ('partial pooling')
# each observed batting average y_i is drawn from a normal density with
# unknown mean theta_i; and fixed variance sigma:
sigma <- sqrt(mean(d$y) * (1-(mean(d$y)))/45)
# Each theta_i (the true batting average of each player; which we
# would observe  if they kept playing forever) is also drawn
# from a normal distribution with unknown mean mu and variance tau.
# Then we give priors to the hyperparameters mu and tau.
# for mu, we know it must be somewhere between 0.15 and .30; so we
# choose values that give a 95% credible interval: normal(0.225, 0.0375)
# see Jackson p.311 for Baseball domain knowledge...
x <- seq(0, 0.5, 0.001)
plot(x, dnorm(x, mean=0.225, sd=0.0375), type="l")
# for tau, we choose a gamma distribution (why? it's always positive
# and it's a conjugate prior for a normal distribution)
# what prior belief do we have about tau,
# the standard deviation of the thetas?
# the sd of the observed y is:
sd(d$y)
# it must be somewhere around 0.07; probably higher; let's say 0.10
# so MEAN of values we draw from gamma distribution must be around 0.10
# gamma distribution has mean shape/rate [OR shape*scale]
# so let's follow Jackman and choose shape=7; then rate=7/0.10 ~= 70
x <- seq(0.001, 1, 0.001)
plot(x, dgamma(x, shape=7, rate=70), type="l")
hierarchical_code <- '
data {
int<lower=0> N; # observations
real y[N]; # outcome variable
real<lower=0> sigma; # fixed variance of observed batting averages
}
parameters {
real theta[N]; ## unobserved true batting averages
real mu; ## hyperparameter: mean of batting averages
real<lower=0> tau; ## hyperparameter: variance of batting averages
}
model {
mu ~ normal(0.225, 0.0375); ## prior about hyperparameter (overall batting average)
tau ~ gamma(7, 70); ## priors about variance of overall batting average
for (n in 1:N){;
y[n] ~ normal(theta[n], sigma);
theta[n] ~ normal(mu, tau);
}
}
'
data <- list(N=length(d$y), y=d$y, sigma=sigma)
fit <- stan(model_code=hierarchical_code, data=data, iter=10000, chains=4)
monitor(fit, digits_summary=3)
library(coda)
mcmc <- stan_to_coda(fit)
geweke.diag(mcmc)
heidel.diag(mcmc)
estimates <- summary(fit)
predicted <- estimates$summary[1:18, 'mean']
rmse_MCMC <- sqrt(mean((predicted-d$p)^2))
rmse_MCMC
actual <- data.frame(batter = d$name, average=d$p, estimate="Actual")
bayes <- data.frame(batter = d$name, average=predicted, estimate="Bayes")
mle <- data.frame(batter = d$name, average=d$y, estimate="MLE")
df <- rbind(actual, bayes, mle)
library(ggplot2)
p <- ggplot(df, aes(x=estimate, y=average, group=batter))
p + geom_point() + geom_line() + theme_bw() + coord_flip() +
scale_x_discrete(expand=c(0.05,0.05)) +
scale_y_continuous("Batting average") +
theme(axis.line.y=element_blank(),
axis.title.y=element_blank(),
axis.ticks.y=element_blank(),
panel.border=element_blank(),
axis.line = element_line(size = 0.50)) +
geom_vline(xintercept=1, alpha=1/3, size=0.2) +
geom_vline(xintercept=2, alpha=1/3, size=0.2) +
geom_vline(xintercept=3, alpha=1/3, size=0.2)
rm(list=ls())
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
# Function to convert stan fit objects to coda objects
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
d <- read.dta("lax_phillips_gay_rights.dta", convert.underscore = TRUE)
getwd()
d <- read.dta("lax_phillips_gay_rights.dta", convert.underscore = TRUE)
setwd("~/Dropbox/Work/Quant_AR/Q32015/Lab/Lab 8/")
d <- read.dta("lax_phillips_gay_rights.dta", convert.underscore = TRUE)
library(foreign)
d <- read.dta("lax_phillips_gay_rights.dta", convert.underscore = TRUE)
str(d)
rm(list=ls())
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
# Function to convert stan fit objects to coda objects
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
data(EfronMorris)
update.packages()
library(rstan)
library(mvtnorm)
library(MASS)
install.packages(c("rstan","mvtnorm"))
library(rstan)
library(mvtnorm)
library(MASS)
hmc_code <- '
data {
vector[2] y; // data (means)
matrix[2,2] Sigma; // covariance matrix (known)
}
parameters {
vector[2] theta; // parameters of bivariate normal
}
model {
y ~ multi_normal(theta, Sigma);
}
'
data <- list(y=y, Sigma=Sigma)
y <- c(0, 0)
rho <- 0.9
Sigma <- matrix(c(1, rho, rho, 1), nrow=2)
hmc_code <- '
data {
vector[2] y; // data (means)
matrix[2,2] Sigma; // covariance matrix (known)
}
parameters {
vector[2] theta; // parameters of bivariate normal
}
model {
y ~ multi_normal(theta, Sigma);
}
'
data <- list(y=y, Sigma=Sigma)
fit <- stan(model_code=hmc_code, data=data, iter=500, chains=4)
monitor(fit)
rm(list=ls())
x <- mvrnorm(n=50, mu=c(-0.5,0,0.5), Sigma=diag(3))
true.beta <- c(-1, 0, 1)
y <- rpois(n=50, lambda=exp(x%*%true.beta))
hmc_code <- '
data {
int<lower=0> N; # observations
int<lower=0> K; # variables
int y[N];       # data (integers)
row_vector[K] x[N];  # covariates
}
parameters {
vector[K] beta;  ## coefficients to be estimated
}
model {
for (k in 1:K)
beta[k] ~ cauchy(0, 2.5); ## priors on betas
for (n in 1:N)
y[n] ~ poisson(exp(x[n] * beta)); ## model for y
}
'
data <- list(N=length(y), K=dim(x)[2], y=y, x=x)
fit <- stan(model_code=hmc_code, data=data, iter=500, chains=4)
monitor(fit)
traceplot(fit, pars='beta')
rm(list=ls())
x <- runif(100)
beta <- 3
alpha <- 1
y <- alpha + beta * x + rnorm(100)
hmc_code <- '
data {
int<lower=0> N; # observations
vector[N] x; # covariate
vector[N] y; # outcome variable
}
parameters {
real alpha;  # intercept
real beta;   # slope
real<lower=0> sigma; # error
}
model {
alpha ~ normal(0, 100);
beta ~ normal(0, 100);
for (n in 1:N)
y[n] ~ normal(alpha + beta * x[n], sigma);
}
'
data <- list(N=length(y), x=x, y=y)
fit <- stan(model_code=hmc_code, data=data, iter=50000, chains=4)
install.packages("coda")
monitor(fit,digits_summary = 3)
summary(lm(y~x))
stan_to_coda <- function(fit){
# Taken From Pablo Barbera
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
# install.packages("coda")
library(coda)
fit_mcmc <- stan_to_coda(fit)
geweke.diag(fit_mcmc)
geweke.plot(fit_mcmc, ask=TRUE)
heidel.diag(fit_mcmc)
geweke.plot(fit_mcmc, ask=TRUE)
autocorr(fit_mcmc)
autocorr.plot(fit_mcmc, ask=TRUE)
?geweke.plot
is.mcmc(fit_mcmc)
geweke.plot(fit_mcmc, ask=TRUE)
fit_mcmc <- stan2coda(fit)
library(coda)
fit_mcmc <- stan2coda(fit)
stan2coda <- function(fit) {
mcmc.list(lapply(1:ncol(fit), function(x) mcmc(as.array(fit)[,x,])))
}
fit_mcmc <- stan2coda(fit)
geweke.plot(fit_mcmc, ask=TRUE)
rm(list=ls())
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
install.packages("pscl")
#-----------------------------------------------------------------------------
# recap:
# The intro showed you how to run a ols and poisson regression in rstan
# now, we are going to do:
# 1. simple partial pooling (no regressors), with informative priors
# 2. Random intercepts model
#-----------------------------------------------------------------------------
# 1. Predicting baseball batting averages via a hierarchical model
# (classic examples that shows Bayesian models outperforming MLE)
# See pages 310-316 Jackman
rm(list=ls())
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
stan_to_coda <- function(fit){
t <- extract(fit, permuted=FALSE, inc_warmup=FALSE)
mcmc <- mcmc.list(lapply(1:ncol(t), function(x) mcmc(t[,x,])))
return(mcmc)
}
data(EfronMorris)
d <- EfronMorris
?EfronMorris
MLE_rmse <- sqrt(mean((d$y-d$p)^2))
MLE_rmse
sigma <- sqrt(mean(d$y) * (1-(mean(d$y)))/45)
x <- seq(0, 0.5, 0.001)
plot(x, dnorm(x, mean=0.225, sd=0.0375), type="l")
sd(d$y)
x <- seq(0.001, 1, 0.001)
plot(x, dgamma(x, shape=7, rate=70), type="l")
hierarchical_code <- '
data {
int<lower=0> N; # observations
real y[N]; # outcome variable
real<lower=0> sigma; # fixed variance of observed batting averages
}
parameters {
real theta[N]; ## unobserved true batting averages
real mu; ## hyperparameter: mean of batting averages
real<lower=0> tau; ## hyperparameter: variance of batting averages
}
model {
mu ~ normal(0.225, 0.0375); ## prior about hyperparameter (overall batting average)
tau ~ gamma(7, 70); ## priors about variance of batting averages
for (n in 1:N){;
y[n] ~ normal(theta[n], sigma);
theta[n] ~ normal(mu, tau);
}
}
'
data <- list(N=length(d$y), y=d$y, sigma=sigma)
fit <- stan(model_code=hierarchical_code, data=data, iter=10000, chains=4)
monitor(fit, digits_summary=3)
# Assessing convergence
# 1) Looking at traceplots
traceplot(fit, pars='theta', inc_warmup=FALSE, ask=TRUE)
library(coda)
# 4) Geweke tests
mcmc <- stan_to_coda(fit)
geweke.diag(mcmc)
geweke.plot(mcmc, ask=TRUE)
# 5) Heidelberger-Welch test of non-stationarity
heidel.diag(mcmc)
# 6) looking at serial correlation in chains
autocorr(mcmc)
autocorr.plot(mcmc, ask=TRUE)
actual <- data.frame(batter = d$name, average=d$p, estimate="Actual")
bayes <- data.frame(batter = d$name, average=predicted, estimate="Bayes")
mle <- data.frame(batter = d$name, average=d$y, estimate="MLE")
df <- rbind(actual, bayes, mle)
library(ggplot2)
p <- ggplot(df, aes(x=estimate, y=average, group=batter))
p + geom_point() + geom_line() + theme_bw() + coord_flip() +
scale_x_discrete(expand=c(0.05,0.05)) +
scale_y_continuous("Batting average") +
theme(axis.line.y=element_blank(),
axis.title.y=element_blank(),
axis.ticks.y=element_blank(),
panel.border=element_blank(),
axis.line = element_line(size = 0.50)) +
geom_vline(xintercept=1, alpha=1/3, size=0.2) +
geom_vline(xintercept=2, alpha=1/3, size=0.2) +
geom_vline(xintercept=3, alpha=1/3, size=0.2)
estimates <- summary(fit)
predicted <- estimates$summary[1:18, 'mean']
rmse_MCMC <- sqrt(mean((predicted-d$p)^2))
rmse_MCMC
# visualizing results (replication of Figure 7.4 in page 316 Jackman)
actual <- data.frame(batter = d$name, average=d$p, estimate="Actual")
bayes <- data.frame(batter = d$name, average=predicted, estimate="Bayes")
mle <- data.frame(batter = d$name, average=d$y, estimate="MLE")
df <- rbind(actual, bayes, mle)
library(ggplot2)
p <- ggplot(df, aes(x=estimate, y=average, group=batter))
p + geom_point() + geom_line() + theme_bw() + coord_flip() +
scale_x_discrete(expand=c(0.05,0.05)) +
scale_y_continuous("Batting average") +
theme(axis.line.y=element_blank(),
axis.title.y=element_blank(),
axis.ticks.y=element_blank(),
panel.border=element_blank(),
axis.line = element_line(size = 0.50)) +
geom_vline(xintercept=1, alpha=1/3, size=0.2) +
geom_vline(xintercept=2, alpha=1/3, size=0.2) +
geom_vline(xintercept=3, alpha=1/3, size=0.2)
rm(list=ls())
library(pscl)
library(foreign)
library(rstan);rstan_options(auto_write = TRUE);options(mc.cores = 3)
d <- read.dta("lax_phillips_gay_rights.dta", convert.underscore = TRUE)
d <- d[!is.na(d$yes.of.all) & !is.na(d$age) & (d$state!="") & !is.na(d$region),]
actual.means <- aggregate(d$yes.of.all, by=list(d$state), mean)$x
age_means <- aggregate(d$age, by=list(d$state), mean)$x
y <- d$yes.of.all
age <- d$age
state <- d$state
states <- sort(unique(state))
ss <- match(state, states) ## number of state to which each obs. belongs
?match
states
state
ss
library(rstan)
